================================================================================
pblk: Physical Block Device Target
================================================================================

pblk is a LightNVM target implementing a full host-side Flash Translation Layer
(FTL). As a target, pblk presents itself as a make_request_fn base driver to the
block layer. In this particular case, as a block device driver, which exposes
the Open-Channel SSD storage device as a block storage to user space. In terms
of target functionality, pblk implements everything related to data placement,
garbage collection (GC) and recovery. In order to access the physical flash,
pblk makes use of LightNVM's generic media manager, which exposes a get/put
block provisioning interface.

By design, pblk supports pluggable mapping and GC algorithms so that a single
hardware device can be accommodated for specific workloads by simply making
changes in software to the desired module (e.g., to the data stripping strategy
in charge of mapping logical to physical addresses).

At the moment, pblk implements the following functionality:

  1. Sector-based host-side translation table management
  2. Write buffering for writing to physical flash pages
  3. Translation table recovery on power outage
  4. Retire bad flash blocks on write failure
  5. Simple cost-based garbage collector (GC)
  6. Sysfs integration
  7. I/O rate-limiting ??

1. Sector-based host-side translation table management
======================================================

 Mapping Strategy
 ----------------


 2. Write Buffer
 ===============
 pblk uses a ring write buffer of size N, where N is the power of two closest to
 the product of the number of active LUNS and the flash block size.

 Buffering facilitates sending writes to the controller in the size of and
 aligned to a given flash page size. A useful facility since constraints by
 controllers include writing aligned and at page granularity. Typically, flash
 pages are between 16KB and 32KB large, and the controller might introduce other
 abstractions that tie pages together (e.g., planes) and make the "apparent"
 page size relatively large (e.g., 64KB, 128KB).

 The flash media introduces additional constraints. Example, for a write to be
 guaranteed persistence on MLC flash, then it must be written in page pairs
 (lower and upper pages). Consequentially, the data in the lower flash page
 cannot be read safely until a write to the upper flash page occurs. Also,
 different types of media might require that, in order to safely read data from
 the media, a certain number of flash pages are written in the same block. The
 consequence is that data must thus remain cached in the host until media
 constrains are met.
 
 We refer to the number of sectors that must reside in the write buffer before
 it is safe to read from the media as the buffers "grace area". We describe
 below how we implement this grace area through a dedicated pointer on the
 write buffer. This problem only aggravates with more complex media such as TLC
 or QLC.

 As a rule of thumb, for a drive with 128 LUNs and average  MLC NAND, the write
 buffer is around 64MBs.

 Buffering writes are thus a means to meet both controller- and media-specific
 constraints.

 Ring Write Buffer Design
 ------------------------
 pblk's ring buffer is implemented as a ring buffer
 (See Documentation/circular-buffers.txt). However, the responsibilities of the
 producer and the consumer are split up into 2 extra pointers. This allows to
 implement the grace area mentioned above. The buffer can be filled and emptied
 at the same time, but both producers and consumers are serialized. We describe
 each pointer separately:

   - Memory Pointer (mem): This is the head pointer (producer). It points to the
     next writable entry on the buffer.

   - Submission Pointer (subm): This is the tail pointer (consumer). It points
     to the next buffered entry that is ready to be submitted to the media.
     The buffer count (number of valid elements in the buffer) is calculated
     using mem and subm.

   - Sync Pointer (sync): It signals the last submitted entry that has
     completed; i.e., that has successfully been persisted to the media. It acts
     as a backpointer that guarantees that I/Os are completed in order. This is
     necessary to guarantee that flushes are completed correctly.
     (See Documentation/block/writeback_cache_control.txt). The buffer space
     (number of free entries on the buffer) is calculate between mem and sync.

   - Flush Pointer (flush): It guarantees that flushes are respected by
     signaling the last entry associated to a REQ_FLUSH request. This pointer is
     taken into consideration by the write thread consuming the buffer (subm);
     if this pointer is present, the write thread

   - Mapping Update Pointer (l2p_update): It guarantees that L2P
     lookups for entries that still reside on the write buffer
     cache will point to it, even if they have successfully been
     persisted to the media. By doing this we guarantee that all
     entries residing in the write buffer will be read from
     cache, thus preventing I/Os to be sent to the device to
     retrieve half-written flash pages. This way, we ensure that
     the whole buffer defines a grace area; the mapping table is
     only updated when the buffer head wraps up.

The write buffer is complemented with a write context buffer, which stores
metadata for each 4KB write. (XXX: Mapping strategy) As mentioned above, we
follow a late-map approach, where the actual mapping to flash pages is done when
the write buffer is being written to the media. This allows us to decouple the
mapping strategy from buffer writes, thus enabling mapping optimizations for
different workloads.

To minimize calculations, pblk's write buffer must be of a power-of-2 size...
also write that it is the closes one to the number of active LUNs in the target
X the size of a flash block.

          mem     l2p_update       sync                        subm
           |          |             |                            |
 -------------------------------------------------------------------------------
|          |          |   synced    |          submitted                       |
-------------------------------------------------------------------------------
| CCCCCCCC | DDDDDDD  | CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC|
 -------------------------------------------------------------------------------

C: L2P points to cache
D: L2P points to device


Write Path
----------
On a new write, the bio associated to it is converted into a write context
(pblk_w_ctx), where the logical block address (LBA), length and flush
requirements are stored.
The bio data is copied to the internal write buffer and completed if a flush is
not required (in case of a flush, the bio is completed after all data on the
write buffer has been persisted to the media). As data is stored on the write
buffer, the translation table is updated so that LBAs point to the buffer
entries data has been stored in.

A separate write thread consumes user data from the write buffer and forms new
bios that are then sent to the device. The mapping strategy described in Section
1 takes place here. This strategy can be modified, as long as controller
constrains are considered. If a flush is required, but there is not enough data
on the write buffer to fulfill these constrains, padding is performed. If the
write succeeds, the sync pointer will be updated on the completion path. When
the head pointer (mem) wraps up and overwrites old data, the l2p_update pointer
takes care of updating the translation table so that future lookups will point
to physical addresses on the device. As mentioned, this late mapping to device
addresses is the mechanism that we use to guarantee the buffer's grace area.

Note writes might fail. Look at Section 4 for an explanation on how to deal with
this case.

Read Path
---------
The read path is simpler than the write path. For each 4KB sector on the read
bio, a lookup is performed to find if data is cached on the write buffer or if
it is necessary to submit an I/O to retrieve it from the physical flash. It
might happen that only some sectors reside in cache. In this case, a new bio is
issue to retrieve sectors from the media, and the original bio is filled out
manually with them.


3. Translation table recovery
=============================
The L2P translation table is maintained entirely in host memory. The ratio
is approximately 1GB of L2P per 1TB of flash. Translation table recovery is
considered for two scenarios:

 (1) Graceful system shutdown (WIP)
 (2) Power failure

In the case of a graceful system shutdown, a L2P snapshot is stored on media.

When an L2P snapshot is not recoverable from media then pblk assumes that an
unexpected system shutdown, such as a power failure, has occurred.
In which case, pblk resorts to scanning the media and thereby reconstruct the
table. Two levels of redundancy are maintained to facilitate reconstruction:

  Firstly, for each 4KB sector, the LBA to physical page address (PPA) mapping
  is stored in the out-of-bound (OOB) area for that sector.

  Secondly, when a block is closed, the last page contains metadata on the block
  itself, including the LBA - PPA mapping list, block status and counters.

All recovery methods leverage functionality of the generic media manager... TODO

4. Write Recovery
=================

5. Garbage Collection
=====================

6. Sysfs integration
====================

7. I/O rate-limiting ??
====================
